import requests
import logging
from typing import List, Dict, Optional

from app.config.config import settings
from app.db.chromadb_manage import get_or_create_collection, query_collection

# Get the main knowledge base collection
knowledge_collection = get_or_create_collection("knowledge_base")
logger = logging.getLogger(__name__)

def get_answer_from_rag(query: str, file_id: Optional[str] = None) -> Dict:
    """
    - retrieves context from the vector DB
    - generates an answer using a local LLM
    """
    context_filter = {"file_id": file_id} if file_id else None

    # retrieve relevant context from chroma db
    results = query_collection(knowledge_collection, [query], n_results=5, where_filter=context_filter)

    relevant_chunks = []
    # results['documents'] is List[List[str]]
    if results and 'documents' in results and results['documents']:
        relevant_chunks = [doc for doc_list in results['documents'] for doc in doc_list if doc is not None]

    if not relevant_chunks:
        return {"answer": "I couldn't find relevant information in the uploaded documents. Please try a different query or upload more documents.", "context_used": []}

    context = "\n\n".join(relevant_chunks)
    
    # Prompt Engineering
    full_prompt = (
        f"You are a helpful assistant. Answer the following question based only on the provided context. "
        f"If the answer is not in the context, clearly state that you don't know or the information is not available.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {query}\n\n"
        f"Answer:"
    )

    try:
        ollama_api_url = f"{settings.OLLAMA_BASE_URL}/api/generate"
        headers = {"Content-Type": "application/json"}
        payload = {
            "model": settings.OLLAMA_LLM_MODEL,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": 0.7
            }
        }

        # the request is made here with the full contexted prompt in the payload
        response = requests.post(ollama_api_url, headers=headers, json=payload, timeout=120)
        response.raise_for_status()

        ollama_response = response.json()
        answer = ollama_response.get("response", "No answer generated by the LLM :(")

        return {"answer": answer, "context_used": relevant_chunks}
    
    # error handling
    except requests.exceptions.ConnectionError:
        logger.error("Error: Could not connect to Ollama.")
        return {"answer": "Error: Could not connect to the local LLM. Please ensure Ollama is running and the model is downloaded.", "context_used": []}
    
    except requests.exceptions.RequestException as e:
        logger.error(f"Error calling Ollama API: {e}")
        return {"answer": f"Error communicating with local LLM: {e}", "context_used": []}
    
    except Exception as e:
        logger.error(f"An unexpected error occurred in RAG service: {e}")
        raise